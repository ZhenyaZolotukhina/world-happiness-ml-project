# -*- coding: utf-8 -*-
"""Копия_блокнота__ML_HW2_team4_ipynb_ (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dyMROIV-7Uf7glAq4wDw1Ra2LyKN-A0J

# Homework 2. Team 4, MLBA group 3.
World Happiness Report 2015–2024: regression task

Eugenia Zolotukhina, Natalia Murchich & Fidan Akhundova

## 1. Import libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression
from mlba import regressionSummary
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, cross_validate
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import TimeSeriesSplit
from sklearn.base import clone

"""## 2. Load dataset"""

df = pd.read_csv('world_happiness_combined.csv', sep = ';', decimal = ',')
df.head()

print('Dataset shape:', df.shape)

print('\nColumn information:')
print(df.info())

print('\nMissing values per column:')
print(df.isnull().sum())

"""## 3. Exploratory data analysis (EDA)"""

# 3.1 Basic statistics for numerical features
df.describe().round(2)

# 3.2 Distribution by year
print('\nDistribution by year:')
print(df['Year'].value_counts().sort_index())

# 3.3 Distribution by region (including missing values)
print('\nDistribution by region (including missing values):')
print(df['Regional indicator'].value_counts(dropna = False))

# 3.4 Correlation heatmap for numerical features
numeric_cols = [
    'Happiness score',
    'GDP per capita',
    'Social support',
    'Healthy life expectancy',
    'Freedom to make life choices',
    'Generosity',
    'Perceptions of corruption',
    'Year'
]

plt.figure(figsize = (8,6))
sns.heatmap(df[numeric_cols].corr(), annot = True, fmt = '.2f', cmap = 'coolwarm')
plt.title('Correlation heatmap for numerical features');

# 3.5 Distribution of Happiness score
plt.figure(figsize = (6,4))
sns.histplot(df['Happiness score'], bins = 20, kde = True)
plt.title('Distribution of Happiness score')
plt.xlabel('Happiness score');

# 3.6 Boxplots for all numerical features
numeric_cols = df.select_dtypes(include = ['number']).columns

n_cols = 3
n_rows = int(np.ceil(len(numeric_cols) / n_cols))

plt.figure(figsize = (5 * n_cols, 4 * n_rows))

for i, col in enumerate(numeric_cols, start=1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(y = df[col])
    plt.title(col)
    plt.xlabel('');

# 3.7 Boxplot of Happiness score by region
plt.figure(figsize = (10,5))
sns.boxplot(data=df, x = 'Regional indicator', y = 'Happiness score')
plt.xticks(rotation = 45, ha = 'right')
plt.title('Happiness score by region');

"""## 4. Data preprocessing and feature selection"""

# Replacement of missing values with regions from geography
country_region_map = {
    'Greece': 'Southern Europe',
    'Cyprus': 'Middle East and North Africa',
    'Gambia': 'West Africa'
}

for country, region in country_region_map.items():
    df.loc[df['Country'] == country, 'Regional indicator'] = region

target_col = 'Happiness score'

numeric_features = [
    'GDP per capita', 'Social support', 'Healthy life expectancy',
    'Freedom to make life choices', 'Generosity', 'Perceptions of corruption',
    'Year', 'Country_encoded'
]

cat_feature = 'Regional indicator'

# Label Encoding for countries
le_country = LabelEncoder()
df['Country_encoded'] = le_country.fit_transform(df['Country'])

scaler = StandardScaler()
X_num_scaled = scaler.fit_transform(df[numeric_features])

X_cat = pd.get_dummies(df[cat_feature], prefix = 'region')

X_num_scaled = pd.DataFrame(
    X_num_scaled,
    columns = numeric_features,
    index = df.index
)

X = pd.concat([X_num_scaled, X_cat], axis = 1)
y = df[target_col]
X

"""## 5. Train–test split"""

# partition data
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size = 0.3,
    random_state = 42
)

"""## 6. Model training"""

# fit model
model = LinearRegression()
model.fit(X_train,y_train)

# prediction train set
train_pred = model.predict(X_train)
train_results = pd.DataFrame({
    'Happiness_score': y_train,
    'predicted': train_pred,
    'residual': y_train - train_pred,
})

train_results.head()

# prediction test set
holdout_pred = model.predict(X_test)
holdout_results = pd.DataFrame({
    'Happiness_score': y_test,
    'predicted': holdout_pred,
    'residual': y_test - holdout_pred,
})
holdout_results.head()

print("Coefficients:")
print("Intercept:", model.intercept_)
for feature, coef in zip(X_num_scaled, model.coef_):
    print(f"{feature}: {coef:.4f}")

# training set
print("\nTraining Set")
regressionSummary(y_true=train_results.Happiness_score, y_pred=train_results.predicted)

# holdout set
print("\nHoldout Set")
regressionSummary(y_true=holdout_results.Happiness_score, y_pred=holdout_results.predicted)

rf_model = RandomForestRegressor(
    n_estimators = 100,
    max_depth = None,
    random_state = 42,
    n_jobs = -1
)

rf_model.fit(X_train, y_train)

y_train_pred_rf = rf_model.predict(X_train)
y_test_pred_rf = rf_model.predict(X_test)

print('Random Forest - train set metrics:')
regressionSummary(y_true = y_train, y_pred = y_train_pred_rf)

print('\nRandom Forest - holdout set metrics:')
regressionSummary(y_true = y_test, y_pred = y_test_pred_rf)

"""## K-fold cross-validation for Linear Regression and Random Forest"""

scoring  =  {
    'neg_RMSE': 'neg_root_mean_squared_error',
    'neg_MAE': 'neg_mean_absolute_error',
    'R2': 'r2',
}

cv  =  KFold(n_splits = 5, shuffle = True, random_state = 42)

# --- Linear Regression: 5-fold CV ---
lin_reg_cv  =  LinearRegression()

cv_results_lr  =  cross_validate(
    lin_reg_cv,
    X_train,
    y_train,
    cv = cv,
    scoring = scoring,
    n_jobs = -1,
    return_train_score = False,
)

rmse_lr = -cv_results_lr['test_neg_RMSE'].mean()
mae_lr = -cv_results_lr['test_neg_MAE'].mean()
r2_lr = cv_results_lr['test_R2'].mean()

# --- Random Forest: 5-fold CV ---
rf_cv  =  RandomForestRegressor(
    n_estimators = 300,
    max_depth = None,
    random_state = 42,
    n_jobs = -1,
)

cv_results_rf  =  cross_validate(
    rf_cv,
    X_train,
    y_train,
    cv = cv,
    scoring = scoring,
    n_jobs = -1,
    return_train_score = False,
)

rmse_rf = -cv_results_rf['test_neg_RMSE'].mean()
mae_rf = -cv_results_rf['test_neg_MAE'].mean()
r2_rf = cv_results_rf['test_R2'].mean()

print('5-fold cross-validation results (on training set)\n')

print('Linear Regression')
print(f'  RMSE (mean over 5 folds): {rmse_lr:.3f}')
print(f'  MAE  (mean over 5 folds): {mae_lr:.3f}')
print(f'  R^2  (mean over 5 folds): {r2_lr:.3f}\n')

print('Random Forest')
print(f'  RMSE (mean over 5 folds): {rmse_rf:.3f}')
print(f'  MAE  (mean over 5 folds): {mae_rf:.3f}')
print(f'  R^2  (mean over 5 folds): {r2_rf:.3f}')

"""## Hyperparameter tuning for Random Forest (optional but useful)"""

param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [None, 8, 12],
    'min_samples_leaf': [1, 3, 5],
}

cv = KFold(n_splits = 5, shuffle = True, random_state = 42)

scoring = {
    'neg_RMSE': 'neg_root_mean_squared_error',
    'neg_MAE': 'neg_mean_absolute_error',
    'R2': 'r2',
}

rf_base = RandomForestRegressor(
    random_state = 42,
    n_jobs = -1,
)

grid_search = GridSearchCV(
    estimator = rf_base,
    param_grid = param_grid,
    scoring = 'neg_root_mean_squared_error',
    cv = cv,
    n_jobs = -1,
    verbose = 1,
)

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_rmse_cv = -grid_search.best_score_

print('Random Forest hyperparameter tuning (5-fold CV on training set)')
print(f'  Best params: {best_params}')
print(f'  Best CV RMSE: {best_rmse_cv:.3f}')

rf_best = grid_search.best_estimator_

y_test_pred_rf_best = rf_best.predict(X_test)

print('\nRandom Forest (tuned)-test set metrics:')
regressionSummary(y_true = y_test, y_pred = y_test_pred_rf_best)

"""## Forecasring happiness score with XGBoost"""

# Creating time features for future forecasting
df = df.sort_values(['Country', 'Year']).reset_index(drop=True)

# Lag features (1, 2, 3 shift)
lag_features = ['Happiness score', 'GDP per capita', 'Social support']
for lag in [1, 2, 3]:
    for feature in lag_features:
        df[f'{feature}_lag_{lag}'] = df.groupby('Country')[feature].shift(lag)

# Moving averages (MA)
for feature in lag_features:
    df[f'{feature}_ma_3'] = df.groupby('Country')[feature].rolling(3, min_periods=1).mean().values

#Preparing numeric and categorical features
numeric_features = [
    'GDP per capita', 'Social support', 'Healthy life expectancy',
    'Freedom to make life choices', 'Generosity', 'Perceptions of corruption',
    'Year', 'Country_encoded',
    # Lags
    'Happiness score_lag_1', 'GDP per capita_lag_1', 'Social support_lag_1',
    'Happiness score_lag_2', 'GDP per capita_lag_2', 'Social support_lag_2',
    # MA
    'Happiness score_ma_3', 'GDP per capita_ma_3', 'Social support_ma_3'
]

cat_feature = 'Regional indicator'

# Removing rows with NaN (caused by lags)
df_model = df.dropna(subset=numeric_features + ['Happiness score']).copy()

# Data preparation
X = df_model[numeric_features]
y = df_model['Happiness score']

# One-Hot for the region
X_cat = pd.get_dummies(df_model[cat_feature], prefix='region')
X = pd.concat([X, X_cat], axis=1)

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""Train/test split"""

split_idx = int(len(X_scaled) * 0.7)
X_train = X_scaled[:split_idx]
X_test = X_scaled[split_idx:]
y_train = y.iloc[:split_idx]
y_test = y.iloc[split_idx:]

print(f"Train: {X_train.shape[0]} observations (first {split_idx} lines)")
print(f"Test: {X_test.shape[0]} observations")

"""Model training"""

model_xgb = xgb.XGBRegressor(n_estimators=100, max_depth=4, random_state=42)
model_xgb.fit(X_train, y_train)

y_pred_train = model_xgb.predict(X_train)
y_pred_test = model_xgb.predict(X_test)

print(f"Train R²: {r2_score(y_train, y_pred_train):.4f}")
print(f"Test R²:  {r2_score(y_test, y_pred_test):.4f}")
print(f"Train RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.4f}")
print(f"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}")

"""Happiness score prediction by 2030"""

# Create list to store future rows
future_rows = []

# Get last year from historical data
last_year = df['Year'].max()

# Define forecast years
future_years = range(last_year + 1, 2031)

countries = df['Country'].unique()

for country in countries:
    df_c = df[df['Country'] == country].sort_values('Year').copy()

    # Generate predictions for each future year
    for year in future_years:
        row = {
            'Country': country,
            'Year': year,
            'Regional indicator': df_c['Regional indicator'].iloc[-1],
        }

        # Linear extrapolation for numerical features
        for col in ['GDP per capita', 'Social support', 'Healthy life expectancy',
                    'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']:

            x = df_c['Year'].values
            ycol = df_c[col].values

            # Handle cases with insufficient variation
            if len(np.unique(ycol)) < 2:
                slope, intercept = 0, ycol[-1]
            else:
                slope, intercept = np.polyfit(x, ycol, 1)

            # Calculating extrapolated value
            row[col] = slope * year + intercept

        # Lag 1 features
        last = df_c.iloc[-1]
        row['Happiness score_lag_1'] = last['Happiness score']
        row['GDP per capita_lag_1'] = last['GDP per capita']
        row['Social support_lag_1'] = last['Social support']

        # Lag 2 features
        if len(df_c) >= 2:
            last2 = df_c.iloc[-2]
            row['Happiness score_lag_2'] = last2['Happiness score']
            row['GDP per capita_lag_2'] = last2['GDP per capita']
            row['Social support_lag_2'] = last2['Social support']
        else:  # Fallback to lag 1 if not enough history
            row['Happiness score_lag_2'] = last['Happiness score']
            row['GDP per capita_lag_2'] = last['GDP per capita']
            row['Social support_lag_2'] = last['Social support']

        # 3-year moving averages
        row['Happiness score_ma_3'] = df_c['Happiness score'].tail(3).mean()
        row['GDP per capita_ma_3'] = df_c['GDP per capita'].tail(3).mean()
        row['Social support_ma_3'] = df_c['Social support'].tail(3).mean()

        # Country encoding
        row['Country_encoded'] = le_country.transform([country])[0]

        # Add row to list
        future_rows.append(row)

future_df = pd.DataFrame(future_rows)

# One-Hot encoding for regions
future_cat = pd.get_dummies(future_df['Regional indicator'], prefix='region')

# Combine features
future_X = pd.concat([future_df[numeric_features], future_cat], axis=1)

# Ensure column alignment with training data
for col in X.columns:
    if col not in future_X.columns:
        future_X[col] = 0

# Match column order to training data
future_X = future_X[X.columns]

# Scaling
future_X_scaled = scaler.transform(future_X)

# Make predictions using XGBoost model
future_df['Predicted_happiness'] = model_xgb.predict(future_X_scaled)

future_df = future_df[['Country', 'Year', 'Predicted_happiness']]

print(future_df.head(20))

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))

for country in future_df['Country'].unique():
    country_data = future_df[future_df['Country'] == country].sort_values('Year')
    plt.plot(country_data['Year'], country_data['Predicted_happiness'], alpha=0.7)

plt.title('Happiness score prediction by 2030', fontsize=14)
plt.xlabel('Yaer')
plt.ylabel('Happiness score')
plt.grid(True, alpha=0.3)
plt.show()

"""## TimeSeriesSplit cross-validation for XGBoost"""

X_cv = X_train
y_cv = y_train

n_splits = 5
tscv = TimeSeriesSplit(n_splits=n_splits)

rmse_folds = []
mae_folds = []
r2_folds = []

print('TimeSeriesSplit (expanding window) for XGBoost on the training set\n')
print(f'Total observations in train: {X_cv.shape[0]}\n')

for fold, (train_idx, val_idx) in enumerate(tscv.split(X_cv), start=1):
    X_tr, X_val = X_cv[train_idx], X_cv[val_idx]
    y_tr, y_val = y_cv.iloc[train_idx], y_cv.iloc[val_idx]

    model_cv = clone(model_xgb)
    model_cv.fit(X_tr, y_tr)

    y_val_pred = model_cv.predict(X_val)

    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    mae = mean_absolute_error(y_val, y_val_pred)
    r2 = r2_score(y_val, y_val_pred)

    rmse_folds.append(rmse)
    mae_folds.append(mae)
    r2_folds.append(r2)

    print(f'Fold {fold}:')
    print(f'  train idx: {train_idx[0]} .. {train_idx[-1]}')
    print(f'  val   idx: {val_idx[0]} .. {val_idx[-1]}')
    print(f'  RMSE = {rmse:.3f}, MAE = {mae:.3f}, R² = {r2:.3f}\n')

print('Average metrics across folds:')
print(f'  RMSE: {np.mean(rmse_folds):.3f}')
print(f'  MAE : {np.mean(mae_folds):.3f}')
print(f'  R²  : {np.mean(r2_folds):.3f}')